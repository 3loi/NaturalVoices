{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./..\")\n",
    "from config import config\n",
    "\n",
    "import torch\n",
    "import json, os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoModelForAudioClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(config['cude_device']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model\n",
    "model = AutoModelForAudioClassification.from_pretrained(config['emo_attributes'], trust_remote_code=True).to(device)\n",
    "\n",
    "#get mean/std\n",
    "mean = model.config.mean\n",
    "std = model.config.std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config['json_path'], 'r') as fp:\n",
    "    all_data = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = glob(os.path.join(config['podcast']['path'], \"*.wav\"))\n",
    "audio.sort()\n",
    "audio = {os.path.basename(x).split('.')[0]:x for x in audio}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, pod in enumerate(tqdm(list(all_data.keys()))):\n",
    "    \n",
    "    wav, sr = librosa.load(audio[pod], sr=16000)\n",
    "\n",
    "    for key, seg in all_data[pod].items():\n",
    "        if 'Happy' in all_data[pod][key]:\n",
    "            continue\n",
    "        if (seg['end'] - seg['start']) > 100:\n",
    "            continue\n",
    "        if len(np.unique(wav[int(seg['start']*sr):int(seg['end']*sr)])) == 1:\n",
    "            continue\n",
    "\n",
    "        x = list(wav[int(seg['start']*sr):int(seg['end']*sr)])\n",
    "        x = torch.tensor(x).unsqueeze(0).to(device)\n",
    "        mask = torch.ones(1, len(x)).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            emo_pred = model(x, mask).cpu().detach().numpy()[0]\n",
    "\n",
    "            \n",
    "        # pred = model()['logits'][0]\n",
    "        all_data[pod][key]['Arousal'] = float(emo_pred[0])\n",
    "        all_data[pod][key]['Dominance'] = float(emo_pred[1])\n",
    "        all_data[pod][key]['Valence'] = float(emo_pred[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config['json_path'], 'w') as fp:\n",
    "    json.dump(all_data, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisperx",
   "language": "python",
   "name": "whisperx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
