{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./..\")\n",
    "from config import config\n",
    "import argparse\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import os, sys\n",
    "from glob import glob\n",
    "import shutil\n",
    "import json\n",
    "import librosa\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from modelsage import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor\n",
    "from transformers.models.wav2vec2.modeling_wav2vec2 import (\n",
    "    Wav2Vec2Model,\n",
    "    Wav2Vec2PreTrainedModel,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model from hub\n",
    "device = torch.device(config['cude_device'])\n",
    "model_name = config[\"age_gender_model\"]\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "model = AgeGenderModel.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config['json_path'], 'r') as fp:\n",
    "    all_data = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_paths = glob(os.path.join(config['podcast']['path'], \"*.wav\"))\n",
    "audio_paths.sort()\n",
    "audio_paths = {os.path.basename(x).split('.')[0]:x for x in audio_paths}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessPipeline(torch.utils.data.Dataset):\n",
    "    def __init__(self, all_data, audios):\n",
    "        print(\"Organizing the data\")\n",
    "        self.X = []\n",
    "        self.segments = []\n",
    "        self.audios = audios\n",
    "        self.data = all_data\n",
    "        for i, pod_name in enumerate(tqdm(list(all_data.keys()))):\n",
    "            for seg_name, seg in all_data[pod_name].items():\n",
    "                if 'age_gender' in seg: continue\n",
    "                if (seg['end']-seg['start']) > 250: continue\n",
    "                \n",
    "                self.X.append([seg['end']-seg['start'], pod_name, seg_name])\n",
    "        \n",
    "        self.X.sort(reverse=False)\n",
    "\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" get a video and its label \"\"\"\n",
    "        _, podcast, seg_name  = self.X[index]\n",
    "        \n",
    "        seg = self.data[podcast][seg_name]\n",
    "\n",
    "        start = seg['start']\n",
    "        end = seg['end']\n",
    "        wav, sr = librosa.load(self.audios[podcast], offset=start, duration=end-start, sr=16000)\n",
    "#         x = test_data_processing(wav, self.mean, self.std)\n",
    "#         x = x.reshape((1, x.shape[0], x.shape[1]))\n",
    "        \n",
    "        return podcast, seg_name, wav\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colate_fun(x):\n",
    "    data = []\n",
    "    lengths = []\n",
    "    for sample in x:\n",
    "        data.append([sample[0], sample[1], np.array(sample[2], dtype=np.float32)])\n",
    "        lengths.append(len(sample[2]))\n",
    "        \n",
    "    aud = np.zeros((len(x), max(lengths)))\n",
    "    for i, a in enumerate(data):\n",
    "        aud[i][:len(a[2])] = a[2]\n",
    "    return data, np.array(aud, dtype=np.float32), lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_data = ProcessPipeline(all_data, audio_paths)\n",
    "process_loader = torch.utils.data.DataLoader(process_data, batch_size = 32, num_workers=2, pin_memory=True, shuffle = False,\n",
    "                                              collate_fn=colate_fun)\n",
    "\n",
    "for info, auds, lengths in tqdm(process_loader):\n",
    "    \n",
    "    outputs = process_func(auds, 16000, processor, model, device)#.item()\n",
    "    \n",
    "    for i, output in zip(info, outputs):\n",
    "        podcast, seg_name, x =  i\n",
    "        buf ={\n",
    "                'Age':float(round(output[0]*100, 1)),\n",
    "                'Female':float(round(output[1], 2)),\n",
    "                'Male':float(round(output[2], 2)),\n",
    "                'Child':float(round(output[3], 2)),\n",
    "            }\n",
    "            \n",
    "        all_data[podcast][seg_name]['age_gender'] = buf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config['json_path'], 'w') as fp:\n",
    "    json.dump(all_data, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisperx",
   "language": "python",
   "name": "whisperx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
