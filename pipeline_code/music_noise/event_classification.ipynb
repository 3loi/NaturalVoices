{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./..\")\n",
    "from config import config\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import os, sys\n",
    "from glob import glob\n",
    "import shutil\n",
    "import json\n",
    "import librosa\n",
    "import torch\n",
    "from transformers import AutoFeatureExtractor, ASTForAudioClassification, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(config['cude_device'])   \n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(config[\"event_classification_model\"])\n",
    "model = ASTForAudioClassification.from_pretrained(config[\"event_classification_model\"])\n",
    "model.to(device)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config['json_path'], 'r') as fp:\n",
    "    all_data = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_paths = glob(os.path.join(config['podcast']['path'], \"*.wav\"))\n",
    "audio_paths.sort()\n",
    "audio_paths = {os.path.basename(x).split('.')[0]:x for x in audio_paths}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessPipeline(torch.utils.data.Dataset):\n",
    "    def __init__(self, all_data, audios):\n",
    "        print(\"Organizing the data\")\n",
    "        self.X = []\n",
    "        self.segments = []\n",
    "        self.audios = audios\n",
    "        self.data = all_data\n",
    "        for i, pod_name in enumerate(tqdm(list(all_data.keys()))):\n",
    "            for seg_name, seg in all_data[pod_name].items():\n",
    "                if 'speech_classification' in seg: continue\n",
    "                if (seg['end']-seg['start']) > 500: continue\n",
    "                \n",
    "                self.X.append([seg['end']-seg['start'], pod_name, seg_name])\n",
    "        \n",
    "        self.X.sort(reverse=False)\n",
    "\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" get a video and its label \"\"\"\n",
    "        _, podcast, seg_name  = self.X[index]\n",
    "        \n",
    "        seg = self.data[podcast][seg_name]\n",
    "\n",
    "        start = seg['start']\n",
    "        end = seg['end']\n",
    "        wav, sr = librosa.load(self.audios[podcast], offset=start, duration=end-start, sr=16000)\n",
    "#         x = test_data_processing(wav, self.mean, self.std)\n",
    "#         x = x.reshape((1, x.shape[0], x.shape[1]))\n",
    "        \n",
    "        return podcast, seg_name, wav\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colate_fun(x):\n",
    "    data = []\n",
    "    lengths = []\n",
    "    for sample in x:\n",
    "        data.append([sample[0], sample[1], np.array(sample[2])])\n",
    "        lengths.append(len(sample[2]))\n",
    "        \n",
    "    aud = np.zeros((len(x), max(lengths)))\n",
    "    for i, a in enumerate(data):\n",
    "        aud[i][:len(a[2])] = a[2]\n",
    "    return data, aud, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_data = ProcessPipeline(all_data, audio_paths)\n",
    "process_loader = torch.utils.data.DataLoader(process_data, batch_size = 32, num_workers=8, pin_memory=True, shuffle = False,\n",
    "                                              collate_fn=colate_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for info, auds, lengths in tqdm(process_loader):\n",
    "    inputs = feature_extractor(auds, sampling_rate=16000, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    \n",
    "    predicted_class_ids = torch.argmax(logits, dim=-1)#.item()\n",
    "    \n",
    "    for i, idx, length in zip(info, predicted_class_ids, lengths):\n",
    "        podcast, seg_name, x =  i\n",
    "        predicted_labels = model.config.id2label[int(idx)] \n",
    "        all_data[podcast][seg_name]['speech_classification'] = predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config['json_path'], 'w') as fp:\n",
    "    json.dump(all_data, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisperx",
   "language": "python",
   "name": "whisperx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
